{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 What is a likelihood function? Also add a formula. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likehood function takes the data set as given,and represents the likeliness of different parameters for our distribution.\n",
    "It measures the support provided by the data for each possible value of the parameter.If we compare the likelihood function at two parameter points and find that L(θ1|x) > L(θ2|x) then the sample we actually observed is more likely to have occurred if θ = θ1 than if θ = θ2. This can be interpreted as θ1 is a more plausible value for θ than θ2.Likelihood function is central to the process of estimating the unknown parameters.Older and less sophisticated methods include the method of moments, and the method of minimum chi-square for count data. These estimators are not always efficient, and\n",
    "their sampling distributions are often mathematically intractable. \n",
    "\n",
    "L(θ |x) = f(x |θ) is likelihood function\n",
    "\n",
    "A Likelihood function is probability of getting certain output depending on the the parameter.\n",
    "The likelihood function is a function of θ.\n",
    "The likelihood function isnota probability density function.\n",
    "The likelihood is only defined up to a constant of proportionality.  In other words, it isan equivalence class of functions.\n",
    "The  likelihood  function  is  used  (i)  to  generate  estimators  (the  maximum  likelihoodestimator) and (ii) as a key ingredient in Bayesian inference.\n",
    "Let X1, X2, ..., Xn  have a joint density function  f(X1, X2, ..., Xn|θ). Given  X1=x1, X2=x2, ..., Xn=xn  is observed, the function ofθdefined by:\n",
    "$$L(θ) =L(θ|x1, x2, ..., xn) =f(x1, x2, ..., xn|θ)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 What is Maximum Likelihood estimation (MLE) ? Can you give an example? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood estimation is that particular assumed value population parameter which maximizes the probability of obtaining the sample data that has been observered.\n",
    "$X1,X2,X3,...Xn$ have joint density denoted $fθ(x1,x2,...,xn) =f(x1,x2,...,xn|θ)$\n",
    "Given observed values $X1=x1,X2=x2,...,Xn=xn$, the likelihood of $θ$ is the function\n",
    "$lik(θ) =f(x1,x2,...,xn|θ)$ considered as a function ofθ.If the distribution is discrete,$f$ will be the frequency distribution function.\n",
    "In words:\n",
    "$lik(θ)$=probability of observing the given data as a function of $θ$.\n",
    "Definition:\n",
    "The maximum likelihood estimate (mle) of $θ$ is that value of $θ$ that maximises $lik(θ)$:  it is the value that makes the observed data the “most probable”.If the $Xi$ are iid, then the likelihood simplifies $$lik(θ) =n∏i=1f(xi|θ)$$\n",
    "Rather than maximising this product which can be quite tedious, we often use the factthat  the  logarithm  is  an  increasing  function  so  it  will  be  equivalent  to  maximise  the  loglikelihood:$$l(θ) =n∑i=1log(f(xi|θ))$$\n",
    "\n",
    "\n",
    "\n",
    "__Example:__<br>\n",
    "Suppose we have a package of seeds, each of which has a constant probability p of success of germination. We plant n of these and count the number of those that sprout. Assume that each seed sprouts independently of the others. How do we determine the maximum likelihood estimator of the parameter p?\n",
    "\n",
    "We begin by noting that each seed is modeled by a Bernoulli distribution with a success of p. We let X be either 0 or 1, and the probability mass function for a single seed is f( x ; p ) = px (1 - p)1 - x. \n",
    "\n",
    "\n",
    "Our sample consists of n  different Xi, each of with has a Bernoulli distribution. The seeds that sprout have Xi = 1 and the seeds that fail to sprout have Xi = 0. \n",
    "\n",
    "The likelihood function is given by:\n",
    "\n",
    "L ( p ) = Π pxi (1 - p)1 - xi\n",
    "\n",
    "We see that it is possible to rewrite the likelihood function by using the laws of exponents. \n",
    "\n",
    "L ( p ) = pΣ xi (1 - p)n - Σ xi\n",
    "\n",
    "Next we differentiate this function with respect to p. We assume that the values for all of the Xi are known, and hence are constant. To differentiate the likelihood function we need to use the product rule along with the power rule:\n",
    "\n",
    "L' ( p ) = Σ xip-1 +Σ xi (1 - p)n - Σ xi - (n - Σ xi )pΣ xi (1 - p)n-1 - Σ xi\n",
    "\n",
    "We rewrite some of the negative exponents and have:\n",
    "\n",
    "L' ( p ) = (1/p) Σ xipΣ xi (1 - p)n - Σ xi - 1/(1 - p) (n - Σ xi )pΣ xi (1 - p)n - Σ xi\n",
    "\n",
    "= [(1/p) Σ xi - 1/(1 - p) (n - Σ xi)]ipΣ xi (1 - p)n - Σ xi\n",
    "\n",
    "Now, in order to continue the process of maximization, we set this derivative equal to zero and solve for p:\n",
    "\n",
    "0 = [(1/p) Σ xi - 1/(1 - p) (n - Σ xi)]ipΣ xi (1 - p)n - Σ xi\n",
    "\n",
    "Since p and (1- p) are nonzero we have that\n",
    "\n",
    "0 = (1/p) Σ xi - 1/(1 - p) (n - Σ xi).\n",
    "\n",
    "Multiplying both sides of the equation by p(1- p) gives us:\n",
    "\n",
    "0 = (1 - p) Σ xi - p (n - Σ xi).\n",
    "\n",
    "We expand the right hand side and see:\n",
    "\n",
    "0 = Σ xi - p Σ xi - p n + pΣ xi = Σ xi - p n.\n",
    "\n",
    "Thus Σ xi = p n and (1/n)Σ xi = p. This means that the maximum likelihood estimator of p is a sample mean. More specifically this is the sample proportion of the seeds that germinated. This is perfectly in line with what intuition would tell us. In order to determine the proportion of seeds that will germinate, first consider a sample from the population of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 How is linear regression related to Pytorch and gradient descent? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a linear approach to modelling the relationship between a dependent variable and one or more independent variables. Let X be the independent variable and Y be the dependent variable. We will define a linear relationship between these two variables as follows:\n",
    "$$Y=mX+c$$\n",
    "$m$ is the slope of the line and $c$ is the y intercept.We will use this equation to train our model with a given dataset and predict the value of $Y$ for any given value of $X$. Our challenge is to determine the value of $m$ and $c$, such that the line corresponding to those values is the best fitting line or gives the minimum error.\n",
    "The loss is the error in our predicted value of $m$ and $c$. Our goal is to minimize this error to obtain the most accurate value of $m$ and $c$.\n",
    "To find the loss function we can use pytorch which has built in libraries which minimizes the number of lines of code.\n",
    "This is all these are related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3 Write out MSE loss for linear regression. Could we also use this loss for classification? "
   ]
  },
  {
   "attachments": {
    "2.jpeg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is the error in our predicted value of m and c. Our goal is to minimize this error to obtain the most accurate value of m and c.\n",
    "We will use the Mean Squared Error function to calculate the loss. There are three steps in this function:\n",
    "Find the difference between the actual y and predicted y value(y = mx + c), for a given x.\n",
    "Square this difference.\n",
    "Find the mean of the squares for every value in X.\n",
    "![](1_3cpC7oHy4IbH3o3Jc-ygVw.jpeg)\n",
    "\n",
    "actual value and ȳᵢ is the predicted value. Lets substitute the value of ȳᵢ:\n",
    "![](attachment:2.jpeg)\n",
    "So we square the error and find the mean. hence the name Mean Squared Error. Now that we have defined the loss function, lets get into the interesting part — minimizing it and finding m and c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Write out the likelihood function for linear classification. What is the drawback of using MSE loss here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logit model the output variable  $y_{i}$ is a Bernoulli random variable (it can take only two values, either 1 or 0)\n",
    "<p> $\\mathrm{P}\\left(y_{i}=1 | x_{i}\\right)=S\\left(x_{i} \\beta\\right)$</p>\n",
    "where\n",
    "$S(t)=\\frac{1}{1+\\exp (-t)}$\n",
    "is the logistic function,\n",
    "$x_{i}$\n",
    "is a\n",
    "$1xK$ vector of inputs and\n",
    "$\\beta $\n",
    "is a  Kx1 vector of coefficients.\n",
    "Furthermore,\n",
    "the likelihood of the entire sample is equal to the product of the likelihoods of the single observations:\n",
    "$L\\left(\\beta ; y_{i}, x_{i}\\right)=\\left[S\\left(x_{i} \\beta\\right)\\right]^{y_{i}}\\left[1-S\\left(x_{i} \\beta\\right)\\right]^{1-y_{i}}$\n",
    "$\\begin{aligned} l(\\beta ; y, X) &=\\ln (L(\\beta ; y, X)) \\\\ &=\\ln \\left(\\prod_{i=1}^{N}\\left[S\\left(x_{i} \\beta\\right)\\right]^{y_{i}}\\left[1-S\\left(x_{i} \\beta\\right)\\right]^{1-y_{i}}\\right) \\\\ &=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(S\\left(x_{i} \\beta\\right)\\right)+\\left(1-y_{i}\\right) \\ln \\left(1-S\\left(x_{i} \\beta\\right)\\right)\\right] \\end{aligned}$\n",
    "$\\begin{array}\n",
    "{=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+\\left(1-y_{i}\\right) \\ln \\left(1-\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right]} \\\\\n",
    "{=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+\\left(1-y_{i}\\right) \\ln \\left(\\frac{1+\\exp \\left(-x_{i} \\beta\\right)-1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right]} \\\\\n",
    "{=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+\\left(1-y_{i}\\right) \\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right]}\n",
    "\\end{array}$\n",
    "$\n",
    "\\begin{array}{l}{=\\sum_{i=1}^{N}\\left[\\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+y_{i}\\left(\\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)-\\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right)\\right]} \\\\ {=\\sum_{i=1}^{N}\\left[\\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)} \\frac{\\exp \\left(x_{i} \\beta\\right)}{\\exp \\left(x_{i} \\beta\\right)}\\right)+y_{i}\\left(\\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)} \\frac{1+\\exp \\left(-x_{i} \\beta\\right)}{\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right\\}\\right]} \\\\ {\\quad=\\sum_{i=1}^{N}\\left[\\ln \\left(\\frac{1}{1+\\exp \\left(x_{i} \\beta\\right)}\\right)+y_{i}\\left(\\ln \\left(\\frac{1}{\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right)\\right]}\\end{array}\n",
    "$\n",
    "$\n",
    "\\begin{array}{l}{=\\sum_{i=1}^{N}\\left[\\ln (1)-\\ln \\left(1+\\exp \\left(x_{i} \\beta\\right)\\right)+y_{i}\\left(\\ln (1)-\\ln \\left(\\exp \\left(-x_{i} \\beta\\right)\\right)\\right]\\right.} \\\\ {=\\sum_{i=1}^{N}\\left[-\\ln \\left(1+\\exp \\left(x_{i} \\beta\\right)\\right)+y_{i} x_{i} \\beta\\right]}\\end{array}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Write out the Maximum likelihood Estimation for linear regression. How is this related to the MSE loss for linear regression derived in the last point? Derive the relation between them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE method can also be used to find the best model parameters of a linear regression model.\n",
    "linear model that needed to be estimated as ŷ.\n",
    "![](1_5OPa4O7PVBrGjfXqDkGnSg.png)\n",
    "![](image1.png)\n",
    "\n",
    "We can consider this ŷ data as also in a normal distribution. But this time, their mean values will be them self since they fall along on top of the ŷ line perfectly. And so the variance of these ŷ data (predicted labels) will be 0. So, ŷ~N(XW , 0)\n",
    "We have an error term called ε (residual) which is the distance between predicted value (ŷ) and actual value(y). And there are some important assumptions that we do in linear regression regarding these residuals, and they are:\n",
    "“ Residuals are normally distributed”\n",
    "“ Residuals have an equal variance”\n",
    "“ Means of residuals are 0”\n",
    "![](image2.png)\n",
    "E(y) = E(ŷ + ε)\n",
    "E(y) = E(ŷ) + E(ε)\n",
    "E(y) = XW + 0 = XW\n",
    "And,\n",
    "Variance(y) = Variance( ŷ + ε )\n",
    "Variance(y) = Variance( ŷ )+ Variance( ε )\n",
    "Variance(y) = 0 + σ²\n",
    "So we can say that y is a normal distribution with mean XW and variance σ².\n",
    "![](image3.png)\n",
    "Now let’s calculate the MLEs for XW and σ² as we did in previous example. But note that here we hava n data points (in earlier example we had only 3), and each of those data point are of dimension d.\n",
    "![](image4.png)\n",
    "As y is a normal distribution,\n",
    "![](image5.png)\n",
    "![](image6.png)\n",
    "![](!1_jjdNcl4ia8wYn2VJGFhGYQ.png)\n",
    "![](!1_IUHvakCuZzY-RCBmDmHAgQ.png)\n",
    "![](1_HJpa51ynuqACIm_qDmikTw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 What are normal equations? Is it the same as least squares? Explain. You may refer to this "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Equation is an analytical approach to Linear Regression with a Least Square Cost Function. We can directly find out the value of θ without using Gradient Descent. Following this approach is an effective and a time-saving option when are working with a dataset with small features.\n",
    "Normal Equation is a follows :\n",
    "![](Untitled-drawing-1-10.png)\n",
    "In the above equation,\n",
    "θ : hypothesis parameters that define it the best.\n",
    "X : Input feature value of each instance.\n",
    "Y : Output value of each instance.</br>\n",
    "\n",
    "__IS normal equation and least squares method same?__\n",
    "\n",
    "__Least Square Method:__\n",
    "\n",
    "The least squares method is a form of mathematical regression analysis that finds the line of best fit for a set of data, providing a visual demonstration of the relationship between the data points. Each point of data is representative of the relationship between a known independent variable and an unknown dependent variable.\n",
    "\n",
    "__Are they same?__\n",
    "\n",
    "No they are not same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Is feature scaling needed for linear regression when using gradient descent?  Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step\n",
    "Yes Feature scaling is needed for linear regression while using gradiendt descent because while using the gradient descent algorithm, feature scaling will help the solution converge in a shorter period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Can gradient descent be used to find the parameters for linear regression? What about linear classification? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent can be used to find the parameters for linear regression because the data points are along the curve so its easy to draw the slope but for linear classification the when the data points are disorted then we cant find the slope we need someother algorithims are needed to find the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Write out the MLE approach for logistic regression. How is this related to the binary cross-entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is widely used to model the outcomes of a categorical dependent variable. For categorical variables it is inappropriate to use linear regression because the response values are not measured on a ratio scale and the error terms are not normally distributed. In addition, the linear regression model can generate as predicted values any real number ranging from negative to positive infinity, whereas a categorical variable can only take on a limited number of discrete values within a specified range.\n",
    "\n",
    "Now let’s consider the logistic model (a binary classifier) to describe log-odds using a linear model:\n",
    "$$\n",
    "\\ln \\frac{p}{1-p}=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{m} x_{m}\n",
    "$$<b>\n",
    "The probability of observing outcome y=1 under this model is given by the following function (sigmoid):<b>\n",
    "$$\n",
    "p \\equiv p(y=1 | \\mathbf{B}, \\mathbf{X})=\\frac{1}{1+e^{-\\left(\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{m} x_{m}\\right)}}\n",
    "$$<b>\n",
    "With 0 and 1 being the only possible outcomes, the probability of observing outcome y=0 is simply (1-p):\n",
    "$$\n",
    "p(y=o | \\mathbf{B}, \\mathbf{X})=p^{o}(1-p)^{1-o}\n",
    "$$\n",
    "The likelihood function is given by the product of all individual probabilities:\n",
    "$$\n",
    "\\mathcal{L}=\\prod_{i=1}^{n} p\\left(y=y_{i} | \\mathbf{B}_{i}, \\mathbf{X}_{i}\\right)=\\prod_{i=1}^{n} p_{i}^{y_{i}}\\left(1-p_{i}\\right)^{1-y_{i}}\n",
    "$$\n",
    "It’s easier to maximize the log-likelihood:\n",
    "$$\n",
    "\\ln \\mathcal{L}=\\sum_{i=1}^{n}\\left(y_{i} \\ln p_{i}+\\left(1-y_{i}\\right) \\ln \\left(1-p_{i}\\right)\\right)\n",
    "$$\n",
    "Thus  maximum liklihood estimation yields a familiar loss function (cross-entropy in this case).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the gradient of the sigmoid analytically. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](IMG_20190806_113604.jpg)\n",
    "![](IMG_20190806_113624.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
